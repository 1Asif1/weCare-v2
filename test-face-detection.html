<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection Test</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        video {
            width: 100%;
            max-width: 640px;
            border: 2px solid #333;
            border-radius: 8px;
        }
        button {
            padding: 10px 20px;
            margin: 10px 5px;
            font-size: 16px;
            cursor: pointer;
        }
        #status {
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            background: #f0f0f0;
        }
        .success { background: #d4edda !important; color: #155724; }
        .error { background: #f8d7da !important; color: #721c24; }
        .info { background: #d1ecf1 !important; color: #0c5460; }
    </style>
</head>
<body>
    <h1>Face Detection Test</h1>
    <p>This page tests if face-api.js models are loading and detecting faces correctly.</p>
    
    <div id="status">Status: Ready</div>
    
    <video id="video" autoplay muted playsinline></video>
    
    <div>
        <button onclick="startTest()">Start Test</button>
        <button onclick="stopTest()">Stop Test</button>
    </div>
    
    <div id="logs" style="margin-top: 20px; padding: 10px; background: #f9f9f9; border-radius: 5px; max-height: 300px; overflow-y: auto;">
        <h3>Logs:</h3>
        <div id="logContent"></div>
    </div>

    <script>
        const video = document.getElementById('video');
        const statusDiv = document.getElementById('status');
        const logContent = document.getElementById('logContent');
        let stream = null;
        let detecting = false;

        function log(message, type = 'info') {
            const time = new Date().toLocaleTimeString();
            const logEntry = document.createElement('div');
            logEntry.textContent = `[${time}] ${message}`;
            logEntry.style.padding = '5px';
            logEntry.style.borderBottom = '1px solid #ddd';
            logContent.appendChild(logEntry);
            logContent.scrollTop = logContent.scrollHeight;
            console.log(message);
        }

        function updateStatus(message, className = '') {
            statusDiv.textContent = 'Status: ' + message;
            statusDiv.className = className;
        }

        async function startTest() {
            try {
                log('Starting face detection test...');
                updateStatus('Loading models...', 'info');
                
                // Load models
                log('Loading face-api.js models from /models/...');
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
                    faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
                    faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
                    faceapi.nets.faceExpressionNet.loadFromUri('/models')
                ]);
                log('✓ Models loaded successfully!', 'success');
                updateStatus('Models loaded. Starting camera...', 'info');
                
                // Start camera
                log('Requesting camera access...');
                stream = await navigator.mediaDevices.getUserMedia({ 
                    video: { width: 640, height: 480 } 
                });
                video.srcObject = stream;
                log('✓ Camera started');
                
                // Wait for video to be ready
                await new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        video.play().then(resolve);
                    };
                });
                log('✓ Video ready and playing');
                updateStatus('Detecting faces...', 'info');
                
                // Start detection loop
                detecting = true;
                detectLoop();
                
            } catch (error) {
                log('✗ Error: ' + error.message, 'error');
                updateStatus('Error: ' + error.message, 'error');
                console.error(error);
            }
        }

        async function detectLoop() {
            if (!detecting) return;
            
            try {
                const detection = await faceapi
                    .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptor();
                
                if (detection) {
                    log('✓ Face detected! Score: ' + detection.detection.score.toFixed(3));
                    updateStatus('Face detected! ✓', 'success');
                } else {
                    updateStatus('No face detected. Position your face in frame.', 'info');
                }
            } catch (error) {
                log('Detection error: ' + error.message, 'error');
                updateStatus('Detection error', 'error');
            }
            
            setTimeout(detectLoop, 500);
        }

        function stopTest() {
            detecting = false;
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                log('Camera stopped');
                updateStatus('Test stopped', 'info');
            }
        }
    </script>
</body>
</html>
